自我们所熟知的AlphaGo横空出世后，强化学习在人工智能方面的重要性日益上升。
强化学习是电脑的强化学习，它的学习主体是agent，agent，通过与环境的交互作用，获得回报。这里的环境可以指在游戏中输出的场景，例如在超级玛丽游戏中，哪里是地面？哪里
有坑？哪里有敌人？哪里有金币等等？这个环境也可以，只对手，比如在围棋游戏中，AlphaGo就是agent而他的对手，就是那个环境。
环境输出状态给agent，agent，接收状态后做出动作，系统会根据这个动作给出回报。比如在超级玛丽游戏中，如果超级玛丽做了一个动作，什么也没有发生，回报就为零，如果这个
动作是吃了金币奖励就为零，如果这个动作是超级玛丽死了，游戏输了奖励为负一万，如果赢了奖励为正一万。一般赢的比赛或输的比赛会比吃到金币或者受伤的回报要大很多很多，因
为游戏的根本目的是赢，而吃金币等只是一个加分项。
在运行时，流程是这样子的，首先agent会接收到一个状态，然后他根据这个状态做出一个动作，做这个动作之后，状态会更新，同时系统会给出奖励。以状态动作奖励新状态为一个循环。
一般来说，结束一场比赛需要很多个动作。因此，会出现很多个这样子的循环。
刚开始的时候，agent，什么都不知道，就跟一张白纸一样，也可以说是一个初学者，而且是那种很笨的初学者，但他的记忆力非常好，还能够记住他做过的所有的动作，以及相应得
到的回报。这样他会重复很多次，然后根据以往经验，判断什么状态下什么动作是最好的，直到得到系统想要的分数，也就是人们想要的结果为止。也就是说，重复的次数越多，电脑
表现得就越好。我想我们打过的王者单机，从青铜到王者，就是电脑训练的次数不同。
在这个过程中，会用到很多函数，动作函数，策略函数。一般一个系统中会运用到多个函数，像阿尔法go运用到了三个。当然有很多种函数，都会有他们自己的缺陷已及改进方法。每一
种情况，比如让机器人下围棋和让机器人说话用到的函数是不一样的，根据情况改变。这一切一般是通过编程来实现。
